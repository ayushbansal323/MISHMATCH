# -*- coding: utf-8 -*-
"""MishMatch baysian forcasting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIGtLfz5sFj2SBlPOw0TSEiG1tT5RZYN
"""

!pip install pymc3

"""# Bayesian Linear Regression

## Exploratory Data Analysis

Kindly note that **Print_Impressions.Ads40** column is changed to **Print_Impressions_Ads40** and **Print_Working_Cost.Ads50** to **Print_Working_Cost_Ads50**
"""

import pymc3 as pm
import pandas as pd
df = pd.read_csv('Training.csv')

df.head()

# This is for coverting data into period
# df.columns
# dfs =  pd.DataFrame(columns=df.columns)
# combine={}
# for col in df.columns:
#   combine[col]=0
# i=0
# for ind in df.index:
#   i=i+1
#   if(i<=28):
#     for col in df.columns:    
#       combine[col]=combine[col]+df[col][ind]
#       #print(df[col][ind],end=' ')
#   else:
#     #print(combine)
#     for col in df.columns:    
#       combine[col]=combine[col]/28
#     dfs=dfs.append(combine, ignore_index=True)
#     for col in df.columns:
#       combine[col]=0
#     i=0


# df=dfs
# df.head()

"""### Distribution of Features

It is userful to find Major Driver of Sales.

We have analysed the distribution of each of the feature.

Performing log transformation to reduce the scale and to distribute the data this will help in increasing the accuracy
"""

import matplotlib.pyplot as plt
import numpy as np

for i in df:
    df[i] = np.log(df[i])

import matplotlib.pyplot as plt
import numpy as np


for i in df:
  plt.hist(df[i], bins = 14)
  plt.xlabel(i)
  plt.ylabel('Time')
  plt.title('Distribution of '+i)
  plt.show()

"""## Corelation

Finding the correlation between every feature with respect to EQ
"""

df.corr()['EQ'].sort_values()

# Drop the features which have low correlation and are not distributed properly
df = df.drop(columns=['Competitor1_RPI',
                      'Competitor2_RPI',
                      'Competitor3_RPI',
                      'Competitor4_RPI',
                      'Fuel_Price',
                      'Avg_EQ_Price',
                      'Median_Temp',
                      'Brand_Equity',
                      'Any_Feat_pct_ACV',
                      'Avg_no_of_Items',
                      'RPI_Category',
                      'Day',])

from sklearn.model_selection import train_test_split
labels = df['EQ']
# df is features and labels are the targets 
# Split by putting 25% in the testing set
X_train, X_test, y_train, y_test = train_test_split(df, labels, 
                                                   test_size = 0.005,
                                                    random_state=42)

# Formula for Bayesian Linear Regression (follows R formula syntax)
formula = 'EQ ~ ' + ' + '.join(['%s' % variable for variable in X_train.columns[1:]])
formula

import pymc3 as pm

# Context for the model
with pm.Model() as normal_model:
    
    # The prior for the data likelihood is a Normal Distribution
    family = pm.glm.families.Normal()
    
    # Creating the model requires a formula and data (and optionally a family)
    pm.GLM.from_formula( formula,data = X_train, family = family)
    
    # Draw the specified number of samples
    normal_trace = pm.sample(draws=500, chains = 1, tune = 100, cores=-1)

import pickle
# save the model to disk
filename = 'finalized_model.sav'
pickle.dump(normal_model, open(filename, 'wb'))

pm.summary(normal_trace)



# Calculate mae and rmse
def evaluate_predictions(predictions, true):
    mae = np.mean(abs(predictions - true))
    rmse = np.sqrt(np.mean((predictions - true) ** 2))
    
    return mae, rmse

model_formula = 'EQ = '
for variable in normal_trace.varnames:
    model_formula += ' %0.2f * %s +' % (np.mean(normal_trace[variable]), variable)

' '.join(model_formula.split(' ')[:-1])

# Evalute the MCMC trace
import matplotlib.pyplot as plt
def evaluate_trace(trace, Xtest, ytest):
    
    # Dictionary of all sampled values for each parameter
    var_dict = {}
    for variable in trace.varnames:
        var_dict[variable] = trace[variable]
        
    # Results into a dataframe
    var_weights = pd.DataFrame(var_dict)

    # Means for all the weights
    var_means = var_weights.mean(axis=0)

    # Create an intercept column
    Xtest['Intercept'] = 1

    # Align names of the test observations and means
    names = Xtest.columns[1:]
    Xtest = Xtest.ix[:, names]

    var_means = var_means[names]

    # Calculate estimate for each test observation using the average weights
    results = pd.DataFrame(index = Xtest.index, columns = ['EQ'],dtype=np.float32)
    
    for row in Xtest.iterrows():
        results.ix[row[0], 'EQ'] = np.dot(np.array(var_means), np.array(row[1]))

    
    # Metrics 
    actual = np.array(ytest)

    errors = np.exp(results['EQ']) - np.exp(actual)
    mae = np.mean(abs(errors))
    rmse = np.sqrt(np.mean(errors ** 2))
    mape = np.mean(np.abs((np.exp(results['EQ']) - np.exp(actual)) /np.exp(actual))) * 100
    
    print('Model  MAE: {:.4f}\nModel RMSE: {:.4f} \nModel MAPE: {:.4f}'.format(mae, rmse, mape))
    print()
    
    lst = [i for i in range(len(actual))]
    plt.plot(lst,np.exp(results['EQ']), label = "predict")
    plt.plot(lst,np.exp(actual), label = "actual")
    plt.legend()
    plt.show()
    return results

all_model_results = evaluate_trace(normal_trace, X_test, y_test)

# Naive baseline is the median 
# This provides the base values of MAE and RMSE
median_pred = X_train['EQ'].median()
median_preds = [median_pred for _ in range(len(X_test))]
true = X_test['EQ']
# Display the naive baseline metrics
mb_mae, mb_rmse = evaluate_predictions(np.exp(median_preds), np.exp(true))
print('Median Baseline  MAE: {:.4f}'.format(mb_mae))
print('Median Baseline RMSE: {:.4f}'.format(mb_rmse))

#Put Your Test Data Here
import pymc3 as pm
import pandas as pd
test = pd.read_csv('Test.csv')

test.head()

# Drop the features which have low correlation and are not distributed properly
test = test.drop(columns=['Competitor1_RPI',
                      'Competitor2_RPI',
                      'Competitor3_RPI',
                      'Competitor4_RPI',
                      'Fuel_Price',
                      'Avg_EQ_Price',
                      'Median_Temp',
                      'Brand_Equity',
                      'Any_Feat_pct_ACV',
                      'Avg_no_of_Items',
                      'RPI_Category',
                      'Period',])

import matplotlib.pyplot as plt
import numpy as np

for i in test:
    test[i] = np.log(test[i])

ytest= np.array(test['EQ'])
ytest = pd.Series(ytest)
all_model_results = evaluate_trace(normal_trace, test, ytest)